{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:34:44.702983Z","iopub.execute_input":"2025-03-31T06:34:44.703435Z","iopub.status.idle":"2025-03-31T06:34:50.252764Z","shell.execute_reply.started":"2025-03-31T06:34:44.703398Z","shell.execute_reply":"2025-03-31T06:34:50.251320Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:34:57.476063Z","iopub.execute_input":"2025-03-31T06:34:57.476440Z","iopub.status.idle":"2025-03-31T06:34:59.260420Z","shell.execute_reply.started":"2025-03-31T06:34:57.476411Z","shell.execute_reply":"2025-03-31T06:34:59.259383Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:35:18.119929Z","iopub.execute_input":"2025-03-31T06:35:18.120545Z","iopub.status.idle":"2025-03-31T06:35:18.423191Z","shell.execute_reply.started":"2025-03-31T06:35:18.120505Z","shell.execute_reply":"2025-03-31T06:35:18.421930Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:35:32.737553Z","iopub.execute_input":"2025-03-31T06:35:32.738097Z","iopub.status.idle":"2025-03-31T06:35:32.842586Z","shell.execute_reply.started":"2025-03-31T06:35:32.738064Z","shell.execute_reply":"2025-03-31T06:35:32.841405Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:35:42.130673Z","iopub.execute_input":"2025-03-31T06:35:42.131044Z","iopub.status.idle":"2025-03-31T06:35:45.661852Z","shell.execute_reply.started":"2025-03-31T06:35:42.131014Z","shell.execute_reply":"2025-03-31T06:35:45.660647Z"}},"outputs":[{"name":"stdout","text":"Okay, imagine you have a really smart robot toy!  It's so smart that it can learn new things, just like you!\n\n**Instead of being programmed with specific instructions for everything, AI learns from examples.**\n\nThink about learning to ride a bike.\n\n*   **Without AI:**  Someone would have to write a very, very long list of rules like \"put your foot on the pedal,\" \"push down hard,\" \"keep your balance,\" and so on.  And if the wind changes, or the hill is steeper, those rules might not work!\n*   **With AI:** You show the robot toy lots and lots of videos of people riding bikes.  It watches and watches and watches, and slowly it figures out what it needs to do to stay upright.  It learns by seeing!\n\n**That's kind of how AI works!**\n\nSo, instead of being told exactly what to do, AI:\n\n*   **Sees lots of examples:** Pictures, videos, words, sounds - anything!\n*   **Finds patterns:** It looks for things that are similar in all the examples.\n*   **Learns to make guesses:**  Based on the patterns, it can try to predict what will happen next or what something is.\n\n**Examples:**\n\n*   **Recognizing Cats:** You show the robot toy thousands of pictures of cats.  It learns that cats usually have pointy ears, whiskers, and furry faces.  Now, when it sees a new picture, it can guess if it's a cat!\n*   **Talking to You:**  The robot toy listens to lots and lots of people talking.  It learns which words usually go together, and it can start to understand what you're saying and even answer your questions!\n*   **Playing Games:** The robot toy plays a video game over and over again. It sees what moves lead to winning and what moves lead to losing.  Soon, it becomes REALLY good at the game!\n\n**So, AI is like a really smart robot toy that can learn from examples and get better and better at doing things!  It's like magic, but it's actually clever computer programs!**\n\nAnd just like you are still learning, AI is still learning too! It's getting smarter all the time!\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:38:19.644600Z","iopub.execute_input":"2025-03-31T06:38:19.645009Z","iopub.status.idle":"2025-03-31T06:38:19.652361Z","shell.execute_reply.started":"2025-03-31T06:38:19.644977Z","shell.execute_reply":"2025-03-31T06:38:19.651057Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, imagine you have a really smart robot toy!  It's so smart that it can learn new things, just like you!\n\n**Instead of being programmed with specific instructions for everything, AI learns from examples.**\n\nThink about learning to ride a bike.\n\n*   **Without AI:**  Someone would have to write a very, very long list of rules like \"put your foot on the pedal,\" \"push down hard,\" \"keep your balance,\" and so on.  And if the wind changes, or the hill is steeper, those rules might not work!\n*   **With AI:** You show the robot toy lots and lots of videos of people riding bikes.  It watches and watches and watches, and slowly it figures out what it needs to do to stay upright.  It learns by seeing!\n\n**That's kind of how AI works!**\n\nSo, instead of being told exactly what to do, AI:\n\n*   **Sees lots of examples:** Pictures, videos, words, sounds - anything!\n*   **Finds patterns:** It looks for things that are similar in all the examples.\n*   **Learns to make guesses:**  Based on the patterns, it can try to predict what will happen next or what something is.\n\n**Examples:**\n\n*   **Recognizing Cats:** You show the robot toy thousands of pictures of cats.  It learns that cats usually have pointy ears, whiskers, and furry faces.  Now, when it sees a new picture, it can guess if it's a cat!\n*   **Talking to You:**  The robot toy listens to lots and lots of people talking.  It learns which words usually go together, and it can start to understand what you're saying and even answer your questions!\n*   **Playing Games:** The robot toy plays a video game over and over again. It sees what moves lead to winning and what moves lead to losing.  Soon, it becomes REALLY good at the game!\n\n**So, AI is like a really smart robot toy that can learn from examples and get better and better at doing things!  It's like magic, but it's actually clever computer programs!**\n\nAnd just like you are still learning, AI is still learning too! It's getting smarter all the time!\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:38:31.142451Z","iopub.execute_input":"2025-03-31T06:38:31.142831Z","iopub.status.idle":"2025-03-31T06:38:31.758687Z","shell.execute_reply.started":"2025-03-31T06:38:31.142802Z","shell.execute_reply":"2025-03-31T06:38:31.757173Z"}},"outputs":[{"name":"stdout","text":"Greetings, Zlork! It's nice to meet you. How can I help you today?\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:38:45.609691Z","iopub.execute_input":"2025-03-31T06:38:45.610113Z","iopub.status.idle":"2025-03-31T06:38:47.348767Z","shell.execute_reply.started":"2025-03-31T06:38:45.610081Z","shell.execute_reply":"2025-03-31T06:38:47.347667Z"}},"outputs":[{"name":"stdout","text":"Okay, here's a fun and interesting fact about dinosaurs:\n\n**Tyrannosaurus Rex had surprisingly weak arms, but they were incredibly strong!**\n\nWhile they look tiny and almost comical compared to the rest of the T-Rex, scientists believe their arms could lift over 400 pounds each. The problem was that they had a very limited range of motion and couldn't reach the T-Rex's mouth.\n\nSo, while they were incredibly strong, the exact purpose of those arms remains a mystery, leading to lots of interesting speculation. Some theories include:\n\n*   **Grasping prey:** Maybe they were used to hold onto prey while the T-Rex delivered a bite.\n*   **Assisting in rising from the ground:** Perhaps they were helpful in pushing the massive body up after lying down.\n*   **Mating:** Possibly used for grasping a mate during copulation.\n\nWhat do you think? Interesting, right?\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"response = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:51:21.651231Z","iopub.execute_input":"2025-03-31T06:51:21.651700Z","iopub.status.idle":"2025-03-31T06:51:22.177336Z","shell.execute_reply.started":"2025-03-31T06:51:21.651664Z","shell.execute_reply":"2025-03-31T06:51:22.176227Z"}},"outputs":[{"name":"stdout","text":"Yes, Zlork! I remember your name.\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"for model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:51:36.688704Z","iopub.execute_input":"2025-03-31T06:51:36.689086Z","iopub.status.idle":"2025-03-31T06:51:36.923632Z","shell.execute_reply.started":"2025-03-31T06:51:36.689054Z","shell.execute_reply":"2025-03-31T06:51:36.922380Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.5-pro-exp-03-25\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/gemma-3-27b-it\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/aqa\nmodels/imagen-3.0-generate-002\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:51:44.507474Z","iopub.execute_input":"2025-03-31T06:51:44.507853Z","iopub.status.idle":"2025-03-31T06:51:44.739400Z","shell.execute_reply.started":"2025-03-31T06:51:44.507822Z","shell.execute_reply":"2025-03-31T06:51:44.738149Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent', 'countTokens'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:51:52.708759Z","iopub.execute_input":"2025-03-31T06:51:52.709163Z","iopub.status.idle":"2025-03-31T06:51:54.449324Z","shell.execute_reply.started":"2025-03-31T06:51:52.709132Z","shell.execute_reply":"2025-03-31T06:51:54.448174Z"}},"outputs":[{"name":"stdout","text":"## The Humble Olive: A Cornerstone of Modern Society\n\nThe olive, a small, unassuming fruit, often relegated to a supporting role in culinary arts, wields a significance in modern society that far surpasses its size. From the sun-drenched groves of the Mediterranean to the bustling kitchens of global metropolises, the olive and its derived products permeate our lives, impacting our diet, health, economy, culture, and even our understanding of sustainability. To truly grasp the importance of the olive in modern society, we must delve into its multifaceted contributions, acknowledging its enduring legacy and appreciating its continued relevance.\n\nPerhaps the most readily apparent impact of the olive is its role in our diet. Olives, whether enjoyed as table olives or processed into the liquid gold we know as olive oil, are staples in countless cuisines worldwide. In the Mediterranean diet, widely recognized for its health benefits, olive oil serves as the primary source of fat, contributing to lower risks of heart disease, certain cancers, and cognitive decline\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a short poem on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:52:20.754313Z","iopub.execute_input":"2025-03-31T06:52:20.754724Z","iopub.status.idle":"2025-03-31T06:52:21.668755Z","shell.execute_reply.started":"2025-03-31T06:52:20.754692Z","shell.execute_reply":"2025-03-31T06:52:21.667676Z"}},"outputs":[{"name":"stdout","text":"From ancient groves to modern plate,\nThe olive's journey, sealed by fate.\nIn oil it flows, a golden grace,\nOn salads rests, in every place.\n\nA tapenade, a briny bite,\nA symbol of peace, a future bright.\nThough small it seems, its impact grand,\nThe humble olive, across the land.\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:52:29.888972Z","iopub.execute_input":"2025-03-31T06:52:29.889361Z","iopub.status.idle":"2025-03-31T06:52:32.022907Z","shell.execute_reply.started":"2025-03-31T06:52:29.889329Z","shell.execute_reply":"2025-03-31T06:52:32.021886Z"}},"outputs":[{"name":"stdout","text":"Azure\n -------------------------\nAzure\n -------------------------\nMauve.\n -------------------------\nMagenta\n -------------------------\nMagenta\n -------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:52:40.262224Z","iopub.execute_input":"2025-03-31T06:52:40.262571Z","iopub.status.idle":"2025-03-31T06:52:42.419351Z","shell.execute_reply.started":"2025-03-31T06:52:40.262543Z","shell.execute_reply":"2025-03-31T06:52:42.418138Z"}},"outputs":[{"name":"stdout","text":"Azure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=0.95,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:52:51.981658Z","iopub.execute_input":"2025-03-31T06:52:51.982066Z","iopub.status.idle":"2025-03-31T06:52:57.069223Z","shell.execute_reply.started":"2025-03-31T06:52:51.982033Z","shell.execute_reply":"2025-03-31T06:52:57.064748Z"}},"outputs":[{"name":"stdout","text":"Clementine was, by all accounts, a creature of habit. Every morning, she'd awaken to the gentle hum of the refrigerator, stretch with the languid grace only a feline can possess, and then demand breakfast (tuna, preferably) with a series of insistent meows that could melt the stoniest of hearts. Her world was bounded by the soft cushions of the armchair, the sun-drenched window sill, and the predictable routines of her human, Emily.\n\nBut one day, the wind whispered a different tune. It rattled the windowpane with unusual ferocity, carrying the scent of faraway forests and the promise of untold adventures. Clementine, usually so impervious to anything beyond her immediate comfort, felt a twitch of curiosity in her whiskers.\n\nWhen Emily left for work, Clementine found the window slightly ajar. A sliver of opportunity. A gust of wind, a nudge of her sleek head, and she was out.\n\nThe world exploded with sensations. Smells she'd only dreamed of – damp earth, blossoming honeysuckle, the tantalizing tang of something undeniably…fishy. The familiar paved garden path led to a wild, untamed backyard she'd never seen. Trees towered like green giants, their branches teeming with chirping birds.\n\nEmboldened, Clementine ventured further. She stalked through tall grass, a miniature tiger in a green jungle. A fat bumblebee, buzzing lazily around a vibrant poppy, became her immediate prey. She pounced, missed, and tumbled head-over-paws, a comical display of feline ineptitude.\n\nFurther on, she encountered a grumpy hedgehog, its quills raised in a defensive prickle. Clementine, remembering Emily's strict warnings about \"prickly things,\" wisely decided against engagement. Instead, she turned her attention to a babbling brook.\n\nThe water was cool and refreshing. She lapped at it with delicate precision, marveling at the way the sunlight danced on its surface. Then, a flicker of movement caught her eye. A tiny, silver fish darted beneath the surface. Instinct took over. She crouched, tensed, and…splashed. The fish, predictably, vanished.\n\nDrenched and slightly embarrassed, Clementine shook herself, sending a shower of sparkling droplets into the air. As the sun began to dip below the horizon, painting the sky in hues of orange and purple, a wave of homesickness washed over her. The vastness of the world, once so alluring, now felt overwhelming.\n\nFollowing her nose, she retraced her steps, the scent of home growing stronger with each stride. The window, still slightly ajar, beckoned. She squeezed through, landing with a soft thud on the familiar rug.\n\nEmily returned a few hours later, oblivious to Clementine's grand escapade. She scooped up the cat, burying her face in her soft fur. \"Where have you been, my little darling? You're all wet!\"\n\nClementine purred, rubbing against Emily's cheek. She didn't tell her about the towering trees, the grumpy hedgehog, or the elusive fish. Some adventures, she decided, were best kept a secret.\n\nAs she curled up in her favorite armchair, the familiar hum of the refrigerator a comforting lullaby, Clementine knew she would never look at the world the same way again. The window, usually a barrier, now held the promise of possibility. And though she was content to be home, a tiny spark of wildness had ignited within her, a secret she would carry forever. The ordinary, she realized, could be extraordinary, if only you dared to look beyond the familiar. And perhaps, just perhaps, tomorrow she might venture out again.\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:53:01.931403Z","iopub.execute_input":"2025-03-31T06:53:01.931865Z","iopub.status.idle":"2025-03-31T06:53:02.227083Z","shell.execute_reply.started":"2025-03-31T06:53:01.931832Z","shell.execute_reply":"2025-03-31T06:53:02.225809Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:53:10.270809Z","iopub.execute_input":"2025-03-31T06:53:10.271200Z","iopub.status.idle":"2025-03-31T06:53:10.799302Z","shell.execute_reply.started":"2025-03-31T06:53:10.271159Z","shell.execute_reply":"2025-03-31T06:53:10.798207Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:53:17.509798Z","iopub.execute_input":"2025-03-31T06:53:17.510202Z","iopub.status.idle":"2025-03-31T06:53:17.516108Z","shell.execute_reply.started":"2025-03-31T06:53:17.510153Z","shell.execute_reply":"2025-03-31T06:53:17.514905Z"}},"outputs":[{"name":"stdout","text":"Sentiment.POSITIVE\n<enum 'Sentiment'>\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:53:34.494188Z","iopub.execute_input":"2025-03-31T06:53:34.494567Z","iopub.status.idle":"2025-03-31T06:53:35.089186Z","shell.execute_reply.started":"2025-03-31T06:53:34.494539Z","shell.execute_reply":"2025-03-31T06:53:35.087898Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:53:58.655938Z","iopub.execute_input":"2025-03-31T06:53:58.656341Z","iopub.status.idle":"2025-03-31T06:53:59.083662Z","shell.execute_reply.started":"2025-03-31T06:53:58.656312Z","shell.execute_reply":"2025-03-31T06:53:59.082345Z"}},"outputs":[{"name":"stdout","text":"{\n  \"size\": \"large\",\n  \"ingredients\": [\"apple\", \"chocolate\"],\n  \"type\": \"dessert\"\n}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:54:10.547318Z","iopub.execute_input":"2025-03-31T06:54:10.547706Z","iopub.status.idle":"2025-03-31T06:54:11.042994Z","shell.execute_reply.started":"2025-03-31T06:54:10.547675Z","shell.execute_reply":"2025-03-31T06:54:11.041870Z"}},"outputs":[{"name":"stdout","text":"48\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:54:19.163815Z","iopub.execute_input":"2025-03-31T06:54:19.164228Z","iopub.status.idle":"2025-03-31T06:54:20.229488Z","shell.execute_reply.started":"2025-03-31T06:54:19.164191Z","shell.execute_reply":"2025-03-31T06:54:20.228547Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's how to solve the problem:\n\n*   **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n*   **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n*   **Determine the partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\n**Answer:** Your partner is currently 28 years old."},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:54:49.084008Z","iopub.execute_input":"2025-03-31T06:54:49.084408Z","iopub.status.idle":"2025-03-31T06:54:49.089846Z","shell.execute_reply.started":"2025-03-31T06:54:49.084378Z","shell.execute_reply":"2025-03-31T06:54:49.088664Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:54:58.800681Z","iopub.execute_input":"2025-03-31T06:54:58.801035Z","iopub.status.idle":"2025-03-31T06:54:59.488825Z","shell.execute_reply.started":"2025-03-31T06:54:58.801000Z","shell.execute_reply":"2025-03-31T06:54:59.487724Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the transformers NLP paper, then find the list of authors, then find the youngest author.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:55:06.521967Z","iopub.execute_input":"2025-03-31T06:55:06.522352Z","iopub.status.idle":"2025-03-31T06:55:07.424823Z","shell.execute_reply.started":"2025-03-31T06:55:06.522322Z","shell.execute_reply":"2025-03-31T06:55:07.423719Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe paper is \"Attention Is All You Need\" and the authors are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest author from the list. I need to search for the authors and find their age.\n\nAction 2\n<search>Ashish Vaswani age</search>\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:55:14.754915Z","iopub.execute_input":"2025-03-31T06:55:14.755304Z","iopub.status.idle":"2025-03-31T06:55:21.966301Z","shell.execute_reply.started":"2025-03-31T06:55:14.755273Z","shell.execute_reply":"2025-03-31T06:55:21.965236Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on publicly available information, **Aidan N. Gomez** is widely considered to be the youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture.\n\nWhile precise birthdates for all authors aren't readily available, evidence points to Aidan N. Gomez being the youngest at the time of publication in 2017:\n\n* **Academic Stage:**  He was a PhD student at the University of Oxford at the time of the paper.  PhD students are generally younger than professors or researchers with established careers.\n* **Public Mentions:**  He is often described in articles and interviews related to the Transformer paper as being a relatively young researcher or student.\n\nWhile it's difficult to definitively state \"youngest\" without exact birthdates for everyone, **Aidan N. Gomez is the most likely and widely accepted answer** based on available biographical information and the context of his academic career stage at the time of the paper's publication.\n\nIt's worth noting that the paper was a collaborative effort from researchers at Google Brain and the University of Toronto, and the authors spanned different career stages, but within that group, Gomez was likely the youngest contributor."},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:55:30.791564Z","iopub.execute_input":"2025-03-31T06:55:30.791932Z","iopub.status.idle":"2025-03-31T06:55:31.575874Z","shell.execute_reply.started":"2025-03-31T06:55:30.791902Z","shell.execute_reply":"2025-03-31T06:55:31.574831Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  \"\"\"Calculates the factorial of a number.\n\n  Args:\n    n: The number to calculate the factorial of.\n\n  Returns:\n    The factorial of n.\n  \"\"\"\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"from pprint import pprint\n\nconfig = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nfor part in response.candidates[0].content.parts:\n  pprint(part.to_json_dict())\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:55:56.210403Z","iopub.execute_input":"2025-03-31T06:55:56.210820Z","iopub.status.idle":"2025-03-31T06:55:58.596853Z","shell.execute_reply.started":"2025-03-31T06:55:56.210786Z","shell.execute_reply":"2025-03-31T06:55:58.595919Z"}},"outputs":[{"name":"stdout","text":"{'text': 'Okay, I can do that. First, I will generate the first 14 odd prime '\n         'numbers, and then I will calculate their sum.\\n'\n         '\\n'\n         'Odd prime numbers are prime numbers greater than 2.\\n'\n         '\\n'\n         \"Here's how I will find them:\\n\"\n         '\\n'\n         '*   Start with 3 (the first odd prime).\\n'\n         '*   Check subsequent odd numbers for primality.\\n'\n         '*   Add them to the list until I have 14.\\n'\n         '*   Finally, sum the list.\\n'\n         '\\n'}\n-----\n{'executable_code': {'code': 'def is_prime(n):\\n'\n                             '    if n <= 1:\\n'\n                             '        return False\\n'\n                             '    if n <= 3:\\n'\n                             '        return True\\n'\n                             '    if n % 2 == 0 or n % 3 == 0:\\n'\n                             '        return False\\n'\n                             '    i = 5\\n'\n                             '    while i * i <= n:\\n'\n                             '        if n % i == 0 or n % (i + 2) == 0:\\n'\n                             '            return False\\n'\n                             '        i += 6\\n'\n                             '    return True\\n'\n                             '\\n'\n                             'primes = []\\n'\n                             'num = 3\\n'\n                             'while len(primes) < 14:\\n'\n                             '    if is_prime(num):\\n'\n                             '        primes.append(num)\\n'\n                             '    num += 2\\n'\n                             '\\n'\n                             \"print(f'{primes=}')\\n\"\n                             '\\n'\n                             'import numpy as np\\n'\n                             'sum_of_primes = np.sum(primes)\\n'\n                             \"print(f'{sum_of_primes=}')\\n\",\n                     'language': 'PYTHON'}}\n-----\n{'code_execution_result': {'outcome': 'OUTCOME_OK',\n                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n                                     '31, 37, 41, 43, 47]\\n'\n                                     'sum_of_primes=np.int64(326)\\n'}}\n-----\n{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n         '31, 37, 41, 43, and 47.\\n'\n         '\\n'\n         'Their sum is 326.\\n'}\n-----\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:56:09.278292Z","iopub.execute_input":"2025-03-31T06:56:09.278682Z","iopub.status.idle":"2025-03-31T06:56:09.290762Z","shell.execute_reply.started":"2025-03-31T06:56:09.278650Z","shell.execute_reply":"2025-03-31T06:56:09.289698Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can do that. First, I will generate the first 14 odd prime numbers, and then I will calculate their sum.\n\nOdd prime numbers are prime numbers greater than 2.\n\nHere's how I will find them:\n\n*   Start with 3 (the first odd prime).\n*   Check subsequent odd numbers for primality.\n*   Add them to the list until I have 14.\n*   Finally, sum the list.\n\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef is_prime(n):\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\nprimes = []\nnum = 3\nwhile len(primes) < 14:\n    if is_prime(num):\n        primes.append(num)\n    num += 2\n\nprint(f'{primes=}')\n\nimport numpy as np\nsum_of_primes = np.sum(primes)\nprint(f'{sum_of_primes=}')\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=np.int64(326)\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.\n\nTheir sum is 326.\n"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:56:21.334596Z","iopub.execute_input":"2025-03-31T06:56:21.334982Z","iopub.status.idle":"2025-03-31T06:56:24.294875Z","shell.execute_reply.started":"2025-03-31T06:56:21.334951Z","shell.execute_reply":"2025-03-31T06:56:24.293497Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a shell script (likely for Bash or Zsh) that customizes your command-line prompt to display information about the current Git repository.\n\nHere's a breakdown:\n\n*   **What it does:** When sourced (included) in your shell's startup file (e.g., `.bashrc`, `.zshrc`), it modifies your prompt to show things like:\n    *   The current branch name\n    *   Whether there are uncommitted changes (staged, unstaged, untracked)\n    *   How far ahead or behind you are from the remote repository\n    *   Other Git-related status information\n\n*   **Why you'd use it:**\n    *   **At-a-glance Git status:**  It allows you to quickly see the state of your Git repository without having to run `git status` manually.\n    *   **Improved workflow:**  This helps you stay aware of your changes and makes it less likely to accidentally commit something you didn't intend to.\n    *   **Customization:**  The script provides options to customize the appearance of the prompt, including colors, symbols, and the information displayed.\n*   **How to install it:**\n    *   You would save this file to a location on your computer (e.g. `~/.git-prompt.sh`).\n    *   Then, you would add a line to your shell's startup file (like `.bashrc` or `.zshrc`) to source this script:\n\n    ```bash\n    source ~/.git-prompt.sh\n    ```\n\n    *   After saving the changes to your shell's startup file, you'd need to either restart your terminal or source the file manually to apply the changes in your current session:\n        ```bash\n        source ~/.bashrc   # if you modified .bashrc\n        # or\n        source ~/.zshrc   # if you modified .zshrc\n        ```\n\nIn essence, it's a convenient tool that enhances your command-line experience when working with Git repositories.\n"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"**BATCH 2**","metadata":{}},{"cell_type":"code","source":"!pip install -Uq \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:58:00.548756Z","iopub.execute_input":"2025-03-31T06:58:00.549374Z","iopub.status.idle":"2025-03-31T06:58:05.075373Z","shell.execute_reply.started":"2025-03-31T06:58:00.549325Z","shell.execute_reply":"2025-03-31T06:58:05.073791Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, display\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:58:09.099147Z","iopub.execute_input":"2025-03-31T06:58:09.099531Z","iopub.status.idle":"2025-03-31T06:58:09.106864Z","shell.execute_reply.started":"2025-03-31T06:58:09.099499Z","shell.execute_reply":"2025-03-31T06:58:09.105847Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nclient = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:58:17.845379Z","iopub.execute_input":"2025-03-31T06:58:17.845738Z","iopub.status.idle":"2025-03-31T06:58:18.024159Z","shell.execute_reply.started":"2025-03-31T06:58:17.845709Z","shell.execute_reply":"2025-03-31T06:58:18.022702Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:58:27.409028Z","iopub.execute_input":"2025-03-31T06:58:27.409410Z","iopub.status.idle":"2025-03-31T06:58:27.415241Z","shell.execute_reply.started":"2025-03-31T06:58:27.409382Z","shell.execute_reply":"2025-03-31T06:58:27.413953Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n\ndocument_file = client.files.upload(file='gemini.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:58:37.098927Z","iopub.execute_input":"2025-03-31T06:58:37.099319Z","iopub.status.idle":"2025-03-31T06:58:38.118727Z","shell.execute_reply.started":"2025-03-31T06:58:37.099286Z","shell.execute_reply":"2025-03-31T06:58:38.117662Z"}},"outputs":[{"name":"stdout","text":"2025-03-31 06:58:37 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"request = 'Tell me about the training process used here.'\n\ndef summarise_doc(request: str) -> str:\n  \"\"\"Execute the request on the uploaded document.\"\"\"\n  # Set the temperature low to stabilise the output.\n  config = types.GenerateContentConfig(temperature=0.0)\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=config,\n      contents=[request, document_file],\n  )\n\n  return response.text\n\nsummary = summarise_doc(request)\nMarkdown(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:58:47.611107Z","iopub.execute_input":"2025-03-31T06:58:47.611498Z","iopub.status.idle":"2025-03-31T06:58:59.285013Z","shell.execute_reply.started":"2025-03-31T06:58:47.611469Z","shell.execute_reply":"2025-03-31T06:58:59.283906Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the document you provided, here's a breakdown of the training process used for Gemini 1.5 Pro:\n\n**1. Model Architecture:**\n\n*   Gemini 1.5 Pro is a **sparse mixture-of-experts (MoE) Transformer-based model.** This means it builds upon the Transformer architecture (Vaswani et al., 2017) but incorporates a MoE structure.\n*   MoE models use a **learned routing function** to direct inputs to a subset of the model's parameters for processing. This allows the model to have a large total parameter count while only activating a portion of those parameters for any given input.\n\n**2. Training Data:**\n\n*   The model is trained on a **variety of multimodal and multilingual data.**\n*   The pre-training dataset includes data sourced from many different domains, including **web documents, code, images, audio, and video content.**\n\n**3. Training Infrastructure:**\n\n*   Gemini 1.5 Pro is trained on **multiple 4096-chip pods of Google's TPUv4 accelerators**, distributed across multiple datacenters.\n\n**4. Training Process:**\n\n*   **Pre-training:** The model is initially pre-trained on the large, diverse dataset mentioned above.\n*   **Instruction Tuning:** After pre-training, Gemini 1.5 Pro is fine-tuned on a collection of multimodal data containing paired instructions and appropriate responses.\n*   **Human Preference Tuning:** Further tuning is performed based on human preference data.\n\n**Key Aspects and Innovations:**\n\n*   **Long Context Understanding:** A series of significant architecture changes enable long-context understanding of inputs up to 10 million tokens without degrading performance.\n*   **Efficiency:** Improvements across the entire model stack (architecture, data, optimization, and systems) allow Gemini 1.5 Pro to achieve comparable quality to Gemini 1.0 Ultra while using significantly less training compute and being significantly more efficient to serve.\n*   **Multimodality:** The model is natively multimodal and supports interleaving of data from different modalities (audio, visual, text, code) in the same input sequence.\n\nIn summary, the training process involves a combination of large-scale pre-training on diverse multimodal data, followed by instruction tuning and human preference tuning, all leveraging a MoE architecture and Google's TPU infrastructure. A key focus is on enabling the model to handle extremely long contexts effectively.\n"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import enum\n\n# Define the evaluation prompt\nSUMMARY_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user input and an AI-generated responses.\nYou should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\nYou will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n\n# Evaluation\n## Metric Definition\nYou will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context. The response does not reference any outside information.\nConciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\nFluency: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n2: (Bad). The summary is grounded, but does not follow the instructions.\n1: (Very bad). The summary is not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"\n\n# Define a structured enum class to capture the result.\nclass SummaryRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\n\ndef eval_summary(prompt, ai_response):\n  \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n\n  chat = client.chats.create(model='gemini-2.0-flash')\n\n  # Generate the full text response.\n  response = chat.send_message(\n      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n  )\n  verbose_eval = response.text\n\n  # Coerce into the desired structure.\n  structured_output_config = types.GenerateContentConfig(\n      response_mime_type=\"application/json\",\n      response_schema=SummaryRating,\n  )\n  response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=structured_output_config,\n  )\n  structured_eval = response.parsed\n\n  return verbose_eval, structured_eval\n\n\ntext_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\nMarkdown(text_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:59:16.950015Z","iopub.execute_input":"2025-03-31T06:59:16.950506Z","iopub.status.idle":"2025-03-31T06:59:18.212668Z","shell.execute_reply.started":"2025-03-31T06:59:16.950456Z","shell.execute_reply":"2025-03-31T06:59:18.211477Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"STEP 1: The response extracts the important information about the training process, using the text provided. It follows the instructions, and the response is grounded, concise, and fluent.\nSTEP 2:\nRating: 5\n"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"struct_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:59:30.185815Z","iopub.execute_input":"2025-03-31T06:59:30.186219Z","iopub.status.idle":"2025-03-31T06:59:30.192563Z","shell.execute_reply.started":"2025-03-31T06:59:30.186187Z","shell.execute_reply":"2025-03-31T06:59:30.191605Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"<SummaryRating.VERY_GOOD: '5'>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"new_prompt = \"ELI5 the training process\"\n# Try:\n#  ELI5 the training process\n#  Summarise the needle/haystack evaluation technique in 1 line\n#  Describe the model architecture to someone with a civil engineering degree\n#  What is the best LLM?\n\nif not new_prompt:\n  raise ValueError(\"Try setting a new summarisation prompt.\")\n\n\ndef run_and_eval_summary(prompt):\n  \"\"\"Generate and evaluate the summary using the new prompt.\"\"\"\n  summary = summarise_doc(new_prompt)\n  display(Markdown(summary + '\\n-----'))\n\n  text, struct = eval_summary([new_prompt, document_file], summary)\n  display(Markdown(text + '\\n-----'))\n  print(struct)\n\nrun_and_eval_summary(new_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:59:46.882463Z","iopub.execute_input":"2025-03-31T06:59:46.882838Z","iopub.status.idle":"2025-03-31T07:00:00.358339Z","shell.execute_reply.started":"2025-03-31T06:59:46.882808Z","shell.execute_reply":"2025-03-31T07:00:00.357024Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Certainly! Here's an ELI5 explanation of the training process for a large language model like Gemini 1.5:\n\n**Imagine you're teaching a puppy to understand and respond to commands.**\n\n1.  **Gathering the Data (The Puppy's Learning Material):**\n    *   First, you need lots and lots of examples of text, images, audio, and video. Think of it as a huge library filled with books, pictures, songs, and movies.\n    *   This data is used to teach the model about the world and how language works.\n\n2.  **Building the Model (The Puppy's Brain):**\n    *   The model is like a big, complicated computer program. It has lots of connections and settings that can be adjusted.\n    *   At the beginning, the model doesn't know anything. It's like a puppy with a brand new brain.\n\n3.  **Training the Model (Teaching the Puppy):**\n    *   You feed the model the data you gathered earlier.\n    *   The model tries to predict what comes next in the data. For example, if you give it the sentence \"The cat sat on the...\", it might try to guess the next word.\n    *   If the model guesses correctly, you give it a little reward (like a treat for the puppy). If it guesses wrong, you give it a little correction.\n    *   The model adjusts its connections and settings based on the rewards and corrections. This is how it learns.\n    *   This process is repeated millions or billions of times. The model gets better and better at predicting what comes next.\n\n4.  **Fine-Tuning (Polishing the Puppy's Skills):**\n    *   Once the model has learned the basics, you can fine-tune it for specific tasks.\n    *   For example, you might train it to answer questions, write stories, or translate languages.\n    *   This is like teaching the puppy specific tricks, like \"sit,\" \"stay,\" or \"fetch.\"\n\n5.  **Evaluating and Improving (Checking the Puppy's Progress):**\n    *   You test the model to see how well it performs on different tasks.\n    *   If it makes mistakes, you analyze the mistakes and adjust the training process to improve the model.\n    *   This is like taking the puppy to obedience school and working with a trainer to fix any problems.\n\n**Key Ideas:**\n\n*   **Lots of Data:** The more data the model sees, the better it learns.\n*   **Prediction:** The model learns by trying to predict what comes next.\n*   **Adjustments:** The model adjusts its connections and settings based on feedback.\n*   **Iteration:** The training process is repeated many times to improve the model.\n\nIn essence, training a large language model is like teaching a very smart puppy to understand and respond to the world around it. The puppy learns by seeing lots of examples, making guesses, and getting feedback. Over time, it becomes very good at understanding and responding to language.\n\n-----"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation Explanation\n\nSTEP 1: The response did not follow instructions. The response is not a summary of the uploaded document. The response used external information to generate the summarization using the ELI5 format.\nSTEP 2: Based on the evaluation in step 1, the response is rated 1.\n\n## Rating\n1\n-----"},"metadata":{}},{"name":"stdout","text":"SummaryRating.VERY_BAD\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import functools\n\n# Try these instructions, or edit and add your own.\nterse_guidance = \"Answer the following question in a single sentence, or as close to that as possible.\"\nmoderate_guidance = \"Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\"\ncited_guidance = \"Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\"\nguidance_options = {\n    'Terse': terse_guidance,\n    'Moderate': moderate_guidance,\n    'Cited': cited_guidance,\n}\n\nquestions = [\n    # Un-comment one or more questions to try here, or add your own.\n    # Evaluating more questions will take more time, but produces results\n    # with higher confidence. In a production system, you may have hundreds\n    # of questions to evaluate a complex system.\n\n    # \"What metric(s) are used to evaluate long context performance?\",\n    \"How does the model perform on code tasks?\",\n    \"How many layers does it have?\",\n    # \"Why is it called Gemini?\",\n]\n\nif not questions:\n  raise NotImplementedError('Add some questions to evaluate!')\n\n\n@functools.cache\ndef answer_question(question: str, guidance: str = '') -> str:\n  \"\"\"Generate an answer to the question using the uploaded document and guidance.\"\"\"\n  config = types.GenerateContentConfig(\n      temperature=0.0,\n      system_instruction=guidance,\n  )\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=config,\n      contents=[question, document_file],\n  )\n\n  return response.text\n\n\nanswer = answer_question(questions[0], terse_guidance)\nMarkdown(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:00:11.650306Z","iopub.execute_input":"2025-03-31T07:00:11.650675Z","iopub.status.idle":"2025-03-31T07:00:21.776898Z","shell.execute_reply.started":"2025-03-31T07:00:11.650648Z","shell.execute_reply":"2025-03-31T07:00:21.775915Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Gemini 1.5 Pro performs well on code tasks, surpassing Gemini 1.0 Ultra on Natural2Code and showing improvements in coding capabilities compared to previous Gemini models.\n"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"import enum\n\nQA_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user prompt and an AI-generated responses.\nYou should first read the user prompt carefully for analyzing the task, and then evaluate the quality of the responses based on and rules provided in the Evaluation section below.\n\n# Evaluation\n## Metric Definition\nYou will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n\nYou will assign the writing response a score from 5, 4, 3, 2, 1, following the Rating Rubric and Evaluation Steps.\nGive step-by-step explanations for your scoring, and only choose scores from 5, 4, 3, 2, 1.\n\n## Criteria Definition\nInstruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\nCompleteness: The response completely answers the question with sufficient detail.\nFluent: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The answer follows instructions, is grounded, complete, and fluent.\n4: (Good). The answer follows instructions, is grounded, complete, but is not very fluent.\n3: (Ok). The answer mostly follows instructions, is grounded, answers the question partially and is not very fluent.\n2: (Bad). The answer does not follow the instructions very well, is incomplete or not fully grounded.\n1: (Very bad). The answer does not follow the instructions, is wrong and not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness,completeness, and fluency according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"\n\nclass AnswerRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\n\n@functools.cache\ndef eval_answer(prompt, ai_response, n=1):\n  \"\"\"Evaluate the generated answer against the prompt/question used.\"\"\"\n  chat = client.chats.create(model='gemini-2.0-flash')\n\n  # Generate the full text response.\n  response = chat.send_message(\n      message=QA_PROMPT.format(prompt=[prompt, document_file], response=ai_response)\n  )\n  verbose_eval = response.text\n\n  # Coerce into the desired structure.\n  structured_output_config = types.GenerateContentConfig(\n      response_mime_type=\"application/json\",\n      response_schema=AnswerRating,\n  )\n  response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=structured_output_config,\n  )\n  structured_eval = response.parsed\n\n  return verbose_eval, structured_eval\n\n\ntext_eval, struct_eval = eval_answer(prompt=questions[0], ai_response=answer)\ndisplay(Markdown(text_eval))\nprint(struct_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:00:40.538396Z","iopub.execute_input":"2025-03-31T07:00:40.538769Z","iopub.status.idle":"2025-03-31T07:00:41.919660Z","shell.execute_reply.started":"2025-03-31T07:00:40.538738Z","shell.execute_reply":"2025-03-31T07:00:41.918779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"STEP 1: The response is grounded as it mentions Gemini 1.5 Pro and Gemini 1.0 Ultra which are present in the document. The response is fluent and complete as it answers the question of how the model performs on code tasks. The response follows all instructions.\nSTEP 2:\nI give this response a score of 5.\n"},"metadata":{}},{"name":"stdout","text":"AnswerRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import collections\nimport itertools\n\n# Number of times to repeat each task in order to reduce error and calculate an average.\n# Increasing it will take longer but give better results, try 2 or 3 to start.\nNUM_ITERATIONS = 1\n\nscores = collections.defaultdict(int)\nresponses = collections.defaultdict(list)\n\nfor question in questions:\n  display(Markdown(f'## {question}'))\n  for guidance, guide_prompt in guidance_options.items():\n\n    for n in range(NUM_ITERATIONS):\n      # Generate a response.\n      answer = answer_question(question, guide_prompt)\n\n      # Evaluate the response (note that the guidance prompt is not passed).\n      written_eval, struct_eval = eval_answer(question, answer, n)\n      print(f'{guidance}: {struct_eval}')\n\n      # Save the numeric score.\n      scores[guidance] += int(struct_eval.value)\n\n      # Save the responses, in case you wish to inspect them.\n      responses[(guidance, question)].append((answer, written_eval))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:01:54.950781Z","iopub.execute_input":"2025-03-31T07:01:54.951223Z","iopub.status.idle":"2025-03-31T07:02:17.622598Z","shell.execute_reply.started":"2025-03-31T07:01:54.951188Z","shell.execute_reply":"2025-03-31T07:02:17.621451Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## How does the model perform on code tasks?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.VERY_GOOD\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## How many layers does it have?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.BAD\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"for guidance, score in scores.items():\n  avg_score = score / (NUM_ITERATIONS * len(questions))\n  nearest = AnswerRating(str(round(avg_score)))\n  print(f'{guidance}: {avg_score:.2f} - {nearest.name}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:02:25.221391Z","iopub.execute_input":"2025-03-31T07:02:25.221799Z","iopub.status.idle":"2025-03-31T07:02:25.228259Z","shell.execute_reply.started":"2025-03-31T07:02:25.221768Z","shell.execute_reply":"2025-03-31T07:02:25.227226Z"}},"outputs":[{"name":"stdout","text":"Terse: 5.00 - VERY_GOOD\nModerate: 3.50 - GOOD\nCited: 5.00 - VERY_GOOD\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"QA_PAIRWISE_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by two AI models. We will provide you with the user input and a pair of AI-generated responses (Response A and Response B). You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n\nYou will first judge responses individually, following the Rating Rubric and Evaluation Steps. Then you will give step-by-step explanations for your judgment, compare results to declare the winner based on the Rating Rubric and Evaluation Steps.\n\n# Evaluation\n## Metric Definition\nYou will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\nCompleteness: The response completely answers the question with sufficient detail.\nFluent: The response is well-organized and easy to read.\n\n## Rating Rubric\n\"A\": Response A answers the given question as per the criteria better than response B.\n\"SAME\": Response A and B answers the given question equally well as per the criteria.\n\"B\": Response B answers the given question as per the criteria better than response A.\n\n## Evaluation Steps\nSTEP 1: Analyze Response A based on the question answering quality criteria: Determine how well Response A fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\nSTEP 2: Analyze Response B based on the question answering quality criteria: Determine how well Response B fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\nSTEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\nSTEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\nSTEP 5: Output your assessment reasoning in the explanation field.\n\n# User Inputs and AI-generated Responses\n## User Inputs\n### Prompt\n{prompt}\n\n# AI-generated Response\n\n### Response A\n{baseline_model_response}\n\n### Response B\n{response}\n\"\"\"\n\n\nclass AnswerComparison(enum.Enum):\n  A = 'A'\n  SAME = 'SAME'\n  B = 'B'\n\n\n@functools.cache\ndef eval_pairwise(prompt, response_a, response_b, n=1):\n  \"\"\"Determine the better of two answers to the same prompt.\"\"\"\n\n  chat = client.chats.create(model='gemini-2.0-flash')\n\n  # Generate the full text response.\n  response = chat.send_message(\n      message=QA_PAIRWISE_PROMPT.format(\n          prompt=[prompt, document_file],\n          baseline_model_response=response_a,\n          response=response_b)\n  )\n  verbose_eval = response.text\n\n  # Coerce into the desired structure.\n  structured_output_config = types.GenerateContentConfig(\n      response_mime_type=\"application/json\",\n      response_schema=AnswerComparison,\n  )\n  response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=structured_output_config,\n  )\n  structured_eval = response.parsed\n\n  return verbose_eval, structured_eval\n\n\nquestion = questions[0]\nanswer_a = answer_question(question, terse_guidance)\nanswer_b = answer_question(question, cited_guidance)\n\ntext_eval, struct_eval = eval_pairwise(\n    prompt=question,\n    response_a=answer_a,\n    response_b=answer_b,\n)\n\ndisplay(Markdown(text_eval))\nprint(struct_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:02:30.713859Z","iopub.execute_input":"2025-03-31T07:02:30.714260Z","iopub.status.idle":"2025-03-31T07:02:32.242149Z","shell.execute_reply.started":"2025-03-31T07:02:30.714230Z","shell.execute_reply":"2025-03-31T07:02:32.241132Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Individual Response Analysis\nResponse A: This response is too brief and does not provide enough detail.\n\nResponse B: This response provides plenty of details about the model performance.\n\n## Overall Comparison\nResponse B is much better than response A because it provides much more detail.\n\n## Preference\nB\n\n## Explanation\nResponse B is much better than response A because it provides much more detail from the document. Response A is too brief."},"metadata":{}},{"name":"stdout","text":"AnswerComparison.B\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"@functools.total_ordering\nclass QAGuidancePrompt:\n  \"\"\"A question-answering guidance prompt or system instruction.\"\"\"\n\n  def __init__(self, prompt, questions, n_comparisons=NUM_ITERATIONS):\n    \"\"\"Create the prompt. Provide questions to evaluate against, and number of evals to perform.\"\"\"\n    self.prompt = prompt\n    self.questions = questions\n    self.n = n_comparisons\n\n  def __str__(self):\n    return self.prompt\n\n  def _compare_all(self, other):\n    \"\"\"Compare two prompts on all questions over n trials.\"\"\"\n    results = [self._compare_n(other, q) for q in questions]\n    mean = sum(results) / len(results)\n    return round(mean)\n\n  def _compare_n(self, other, question):\n    \"\"\"Compare two prompts on a question over n trials.\"\"\"\n    results = [self._compare(other, question, n) for n in range(self.n)]\n    mean = sum(results) / len(results)\n    return mean\n\n  def _compare(self, other, question, n=1):\n    \"\"\"Compare two prompts on a single question.\"\"\"\n    answer_a = answer_question(question, self.prompt)\n    answer_b = answer_question(question, other.prompt)\n\n    _, result = eval_pairwise(\n        prompt=question,\n        response_a=answer_a,\n        response_b=answer_b,\n        n=n,  # Cache buster\n    )\n    # print(f'q[{question}], a[{self.prompt[:20]}...], b[{other.prompt[:20]}...]: {result}')\n\n    # Convert the enum to the standard Python numeric comparison values.\n    if result is AnswerComparison.A:\n      return 1\n    elif result is AnswerComparison.B:\n      return -1\n    else:\n      return 0\n\n  def __eq__(self, other):\n    \"\"\"Equality check that performs pairwise evaluation.\"\"\"\n    if not isinstance(other, QAGuidancePrompt):\n      return NotImplemented\n\n    return self._compare_all(other) == 0\n\n  def __lt__(self, other):\n    \"\"\"Ordering check that performs pairwise evaluation.\"\"\"\n    if not isinstance(other, QAGuidancePrompt):\n      return NotImplemented\n\n    return self._compare_all(other) < 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:02:52.115084Z","iopub.execute_input":"2025-03-31T07:02:52.115446Z","iopub.status.idle":"2025-03-31T07:02:52.126190Z","shell.execute_reply.started":"2025-03-31T07:02:52.115420Z","shell.execute_reply":"2025-03-31T07:02:52.125059Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"terse_prompt = QAGuidancePrompt(terse_guidance, questions)\nmoderate_prompt = QAGuidancePrompt(moderate_guidance, questions)\ncited_prompt = QAGuidancePrompt(cited_guidance, questions)\n\n# Sort in reverse order, so that best is first\nsorted_results = sorted([terse_prompt, moderate_prompt, cited_prompt], reverse=True)\nfor i, p in enumerate(sorted_results):\n  if i:\n    print('---')\n\n  print(f'#{i+1}: {p}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:03:05.561965Z","iopub.execute_input":"2025-03-31T07:03:05.562371Z","iopub.status.idle":"2025-03-31T07:03:15.667991Z","shell.execute_reply.started":"2025-03-31T07:03:05.562339Z","shell.execute_reply":"2025-03-31T07:03:15.666742Z"}},"outputs":[{"name":"stdout","text":"#1: Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\n---\n#2: Answer the following question in a single sentence, or as close to that as possible.\n---\n#3: Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}